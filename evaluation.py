#
# File name: evaluation.py
# Author: Danique van der Wijk
# Student number: s3989771
# Last updated: 3 February 2025
# Description: The file with the code to execute the Support Vector Machine (SVM) and the calculation of the BLEU scores
#


from nltk.translate.bleu_score import sentence_bleu
from transformers import AutoTokenizer, AutoModelForCausalLM
import Levenshtein
import torch
import spacy

nlp = spacy.load("nl_core_news_sm")


def calculate_bleu_score(stego_text, cover_text, weights):
    """
    Calculate the BLEU score between the stego text and cover text.
    Args:
        stego_text (str): The text generated by the steganography algorithm.
        cover_text (str): The original text used as a cover.
        weights (tuple): The weights for the BLEU score calculation, typically a tuple of four values.
    Returns:
        float: The average BLEU score for the sentences in the stego text compared to the cover text.
    Raises:
        ValueError: If the number of sentences in the stego text and cover text are not the same.
    """
    # Split the input texts into sentences
    stego_sentences = stego_text.split(".")
    cover_sentences = cover_text.split(".")

    # Check if the number of sentences is the same
    if len(stego_sentences) != len(cover_sentences):
        raise ValueError("The number of sentences in the stego text and cover text must be the same.")

    bleu_scores = []

    # Calculate BLEU score for each sentence
    for cover_sentence, stego_sentence in zip(cover_sentences, stego_sentences):
        reference = cover_sentence.split()
        candidate = stego_sentence.split()
        score = sentence_bleu([reference], candidate, weights=weights)
        bleu_scores.append(score)
    
    # Calculate average BLEU score
    avg_bleu_score = sum(bleu_scores) / len(bleu_scores)
    #print(f"Average BLEU score: {round(avg_bleu_score, 3)}")

    return avg_bleu_score


def perplexity(text: str, model_name: str = "gpt2", stride: int = 512) -> float:
    """
    Calculate the perplexity of a given text using a specified language model.
    Perplexity is a measurement of how well a probability model predicts a sample.
    A lower perplexity indicates the model is better at predicting the sample.
    Args:
        text (str): The input text for which to calculate perplexity.
        model_name (str, optional): The name of the pre-trained language model to use. Defaults to "gpt2".
        stride (int, optional): The stride size for processing the text in chunks. Defaults to 512.
    Returns:
        float: The calculated perplexity of the input text.
    """
    # Load tokenizer and model
    tokenizer = AutoTokenizer.from_pretrained(model_name)
    model = AutoModelForCausalLM.from_pretrained(model_name)
    model.eval()

    # Encode text
    encodings = tokenizer(text, return_tensors='pt')

    max_length = model.config.n_positions
    input_ids = encodings.input_ids
    seq_len = input_ids.size(1)

    nlls = []
    # process the text in parts of the size 'stride'
    for i in range(0, seq_len, stride):
        begin_loc = i
        end_loc = min(i + stride, seq_len)

        # For the first segment, the full length is taken as target
        # For the other segments we only count the new tokens
        trg_len = end_loc - (begin_loc if i > 0 else 0)

        input_ids_slice = input_ids[:, begin_loc:end_loc]
        target_ids = input_ids_slice.clone()

        # Make sure only the tokens we added in this batch are counted in the loss
        if i > 0:
            target_ids[:, :-trg_len] = -100

        with torch.no_grad():
            outputs = model(input_ids_slice, labels = target_ids)
            neg_log_likelihood = outputs.loss * trg_len

        nlls.append(neg_log_likelihood)
    
    # Perplexity = exp(NLL / token count)
    ppl = torch.exp(torch.stack(nlls).sum() / seq_len)
    return ppl.item()

def levenshtein_dist(word1, word2):
    """
    Calculate the Levenshtein distance between two words.

    The Levenshtein distance is a measure of the difference between two sequences.
    It is the minimum number of single-character edits (insertions, deletions, or substitutions)
    required to change one word into the other.

    Parameters:
    word1 (str): The first word to compare.
    word2 (str): The second word to compare.

    Returns:
    int: The Levenshtein distance between the two words.
    """
    distance = Levenshtein.distance(word1, word2)
    return distance
